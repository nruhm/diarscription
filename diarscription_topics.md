# 1. Transcriptions

Transcriptions involve implementing OpenAI's Whisper automatic speech recognition model to convert audio files into text transcriptions. Preprocessed files should be used (16kHz sample rate, noise reduction), so formatting the file beforehand is smart. The expected results are highly accurate transcriptions with at least 90% accuracy on clear audio recordings. The base model provides a good balance between speed and accuracy, though larger models like small or medium can be used for better results at the cost of processing time. Whisper handles multiple languages and can automatically detect the spoken language, making it versatile for various meeting contexts. The transcription process returns segments with start times, end times, and confidence scores for each detected phrase.

# 2. Number of Speakers

To obtain the number of speakers, you can use pyannote speaker diarization via pyannote. You should use the same processed file from transcription throughout every step to ensure that the format is the same. The expected result is a diarization report, with information such as speakers, time stamps, and other parameters that you can leave on or off such as gender. The pyannote.audio pipeline uses neural networks trained on large datasets to distinguish between different voice characteristics and speaking patterns. The speaker-diarization-3.1 model requires authentication through Hugging Face and can leverage GPU acceleration when available. The pipeline returns an annotation object that can be iterated through to access each speaker segment with corresponding labels like SPEAKER_00, SPEAKER_01, etc.

# 3. When Speakers Spoke

You can use the information given from pyannote to also determine when speakers spoke. Using pyannote, one of the pieces of information given is the time between each sentence. Although pyannote by itself doesn't pr ovide the text of what the person spoke, it still provides accurate time stamps that can be used. The diarization output includes turn objects with start and end attributes that precisely mark when each speaker began and stopped talking. These timestamps can then be cross-referenced with Whisper's transcription segments to match spoken words with specific speakers. The itertracks method allows iteration through all speaking turns in chronological order, yielding the time interval, track identifier, and speaker label for each segment.

# 4. Speaker overlap ranges when multiple people talk 

My understanding is that pyannote detects when multiple people are talking, and breaks it into multiple speakers, saving the frequency ranges for each speaker. To manually view this, you can see where there are large changes in the audio file via audacity or other audio programs. The model uses voice activity detection (VAD) to identify speech regions and then clusters similar voice embeddings to assign speaker labels. When overlap occurs, pyannote can segment the audio into overlapping time windows and assign multiple speaker labels to the same time period. This capability is crucial for natural conversations where participants may interrupt or speak simultaneously, though the current implementation prioritizes non-overlapping segments for cleaner transcription output.

# 5. Separate Speakers

This involves implementing pyannote's speech separation model pyannote.ami to isolate individual speakers from an audio file with multiple files. This requires configuration of the separation pipeline, processing audio files to extract distinct channels, and exporting each file with the speaker's name in the file. The expected results should include separate WAV files for each speaker, and the audio quality should be preserved/clean. The separation process uses source separation algorithms to unmix the audio signal into individual speaker tracks based on learned voice characteristics. Each separated file maintains the original timeline but contains only the audio attributed to that specific speaker, with other voices suppressed or removed. This approach enables parallel processing of individual speaker audio through subsequent transcription steps.

# 6. Match speakers in range, match number of speakers detected 

This involves setting two variables, one before and after implementing pyannote.ami. My approach to this is getting the amount of speakers from pyannote, and making sure it matches with the amount of files created via pyannote.ami. The result should be an if statement that checks if the files "SPEAKER_00, SPEAKER_01, etc" are found in the files where the speaker names are found via pyannote. This validation step ensures data integrity throughout the pipeline and catches any discrepancies between diarization and separation outputs. The speaker count can be extracted by collecting unique speaker labels from the diarization annotation object. A simple verification loop can compare the set of detected speakers against the list of generated audio files to confirm complete coverage.

# 7. Create audio clips for each speaker

My understanding is that step 5 "separate speakers" already broke up each speaker into their own audio clips. This step may take those files and format them, or change the file type to wav or another desired file type. The results of this step should be the audio clips created for each speaker of the desired file type (mp3, wav, etc) and these files should be ready to be accessed by whisperx. Additional processing might include normalizing audio levels, trimming silence from beginning and end of files, or resampling to ensure consistent format. The soundfile and librosa libraries provide utilities for reading, writing, and converting between different audio formats. These prepared clips serve as optimized input for the next stage of speaker-specific transcription.

# 8. Run whisperx through the created audio files 

Whisperx has to then go through the audio files created by pyannote.ami and obtain enhanced transcriptions from each speaker. The expected results from this extraction should include highly accurate transcription information for each individual speaker. WhisperX extends the base Whisper model with forced alignment capabilities, providing word-level timestamps rather than just segment-level timing. This enhanced precision allows for better synchronization between the transcribed text and the original audio timeline. By processing each speaker's isolated audio separately, WhisperX can achieve higher accuracy since it doesn't have to handle multiple overlapping voices in the same pass.

# 9. Get whisperx data into an array (token, start, end, speaker)

After you run whisperx through each audio file created by pyannote.ami, you then have to get the array information (token, start time, end time, speaker #). The results should be an array saved to a variable that contains those values, and you should be able to access them at the end of the program. The WhisperX output includes word-level segments with precise timing information that can be parsed into a structured data format. Each entry in the array represents a single word or token with its associated metadata, allowing for granular reconstruction of the conversation. This structured format facilitates downstream processing such as searching for specific words, analyzing speaking patterns, or generating various output formats like SRT or JSON.

# 10. Whisperx data into txt file 

Lastly, you need to get the token data from WhisperX, which contains the timestamps from each speaker, and write them into a txt file. The results of this should be a JSON file that is properly formatted and it should have the finished transcription, from start to finish, with all speakers present and everything said being properly in place. The output format should be human-readable while maintaining machine-parseable structure for potential future processing or integration with other systems. JSON format provides flexibility for storing nested data structures including metadata like confidence scores, speaker labels, and timing information. The final file serves as both a transcript for human review and a data source for analytics, searchability, or integration with meeting management platforms.